# -*- coding: utf-8 -*-
"""Llama3_Training_Final

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1481vSAVkxHvUNQTQmTKd3SC_nfs2KXRV
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# %pip install -U bitsandbytes
# %pip install -U transformers
# %pip install -U accelerate
# %pip install -U peft
# #%pip install -U trl

pip install trl==0.12.0

import numpy as np
import pandas as pd
import os
from tqdm import tqdm
import bitsandbytes as bnb
import torch
import torch.nn as nn
import transformers
from datasets import Dataset
from peft import LoraConfig, PeftConfig
from trl import SFTTrainer
from trl import setup_chat_format
from sklearn.model_selection import train_test_split

from transformers import (AutoModelForCausalLM,
                          AutoTokenizer,
                          BitsAndBytesConfig,
                          TrainingArguments,
                          pipeline,
                          logging)
from sklearn.metrics import (accuracy_score,
                             classification_report,
                             confusion_matrix)

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive/MyDrive/'CS 7650'

df = pd.read_excel('reddit_data.xlsx')

# Report the number of sentences.
print('Number of training sentences: {:,}\n'.format(df.shape[0]))

# Display 10 random rows from the data.
df.sample(10)

df.shape

df['Title'] = df['Title'].fillna('')
df['Text'] = df['Text'].fillna('')
df['Full_Text'] = df['Title'] + ' ' + df['Text']
df['Full_Text'] = df['Full_Text'].astype(str)
df['Label'] = np.where(df['Label'] == 'anorexia', 'eating disorder', df['Label'])
df_subset1 = df[['ID', 'Full_Text','Label']]
df_subset1 = df_subset1[0:5000]
df_subset1 = df_subset1.rename(columns={'Full_Text' : 'text'})
df_subset1 = df_subset1.rename(columns={'text' : 'statement', 'Label': 'status'})
df_subset1.head()

df_subset1.shape

#Create a function to clean text in the datafrmae
def clean_text(text):

  # Remove emojis
  text = re.sub(r'[:;][\w\d-]*[:;]', '', text)

  # Remove emoticons
  text = re.sub(r'\([^\)]*\)', '', text)

  # Remove hyperlinks
  text = re.sub(r'https?://[^\s]+', '', text)

  # Remove non-English characters
  text = re.sub(r'[^\w\s]', '', text)

  # Convert all text into lowercase
  text = text.lower()

  return text

import re
df_subset1['statement'] = df_subset1['statement'].apply(clean_text)
df_subset1.sample(10)

import sklearn
from sklearn.model_selection import train_test_split
train_df, test_df = train_test_split(df_subset1, test_size=0.20, random_state=123)

def generate_prompt(data_point):
    return f"""
            Classify the text into none, depression, anxiety, bipolar, ptsd, schizophrenia, eating disorder, or OCD and return the answer as the corresponding mental health disorder label.
text: {data_point["statement"]}
label: {data_point["status"]}""".strip()

def generate_test_prompt(data_point):
    return f"""
            Classify the text into none, depression, anxiety, bipolar, ptsd, schizophrenia, eating disorder, or OCD and return the answer as the corresponding mental health disorder label.
text: {data_point["statement"]}
label: """.strip()

X_train = train_df.copy()
X_test = test_df.copy()
X_train.loc[:,'text'] = X_train.apply(generate_prompt, axis=1)

X_train

y_true = X_test.loc[:,'status']
X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=["text"])

X_train.status.value_counts()

y_true.value_counts()

train_data = Dataset.from_pandas(X_train[["text"]])
train_data['text'][3]

midpoint = len(X_test) // 2
eval_data = Dataset.from_pandas(X_test[["text"]][:midpoint])

eval_data[0:5]

eval_data.shape

# Commented out IPython magic to ensure Python compatibility.
# %pip install transformers bitsandbytes

!pip install llama-stack

!huggingface-cli login

from transformers import AutoModelForCausalLM
from transformers import AutoTokenizer

pip install -U bitsandbytes

from transformers import AutoModelForCausalLM
from transformers import BitsAndBytesConfig

base_model_name = "meta-llama/Llama-3.1-8B-Instruct"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=False,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype="float16",
)

model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    device_map="auto",
    torch_dtype="float16",
    quantization_config=bnb_config,
)

model.config.use_cache = False
model.config.pretraining_tp = 1

tokenizer = AutoTokenizer.from_pretrained(base_model_name)

tokenizer.pad_token_id = tokenizer.eos_token_id

def predict(test, model, tokenizer):
    y_pred = []
    categories = ["none", "depression", "anxiety", "bipolar", "eating disorder", "OCD", "ptsd", "schizophrenia"]

    for i in tqdm(range(len(test))):
        prompt = test.iloc[i]["text"]
        pipe = pipeline(task="text-generation",
                        model=model,
                        tokenizer=tokenizer,
                        max_new_tokens=2,
                        temperature=0.1)

        result = pipe(prompt)
        answer = result[0]['generated_text'].split("label:")[-1].strip()

        # Determine the predicted category
        for category in categories:
            if category.lower() in answer.lower():
                y_pred.append(category)
                break
        else:
            y_pred.append("none")

    return y_pred

X_test.shape

y_pred = predict(X_test[0:100], model, tokenizer)

def evaluate(y_true, y_pred):
    labels = ["none", "depression", "anxiety", "bipolar", "eating disorder", "OCD", "ptsd", "schizophrenia"]
    mapping = {label: idx for idx, label in enumerate(labels)}

    def map_func(x):
        return mapping.get(x, -1)

    y_true_mapped = np.vectorize(map_func)(y_true)
    y_pred_mapped = np.vectorize(map_func)(y_pred)


    accuracy = accuracy_score(y_true=y_true_mapped, y_pred=y_pred_mapped)
    print(f'Accuracy: {accuracy:.3f}')


    unique_labels = set(y_true_mapped)

    for label in unique_labels:
        label_indices = [i for i in range(len(y_true_mapped)) if y_true_mapped[i] == label]
        label_y_true = [y_true_mapped[i] for i in label_indices]
        label_y_pred = [y_pred_mapped[i] for i in label_indices]
        label_accuracy = accuracy_score(label_y_true, label_y_pred)
        print(f'Accuracy for label {labels[label]}: {label_accuracy:.3f}')


    class_report = classification_report(y_true=y_true_mapped, y_pred=y_pred_mapped, target_names=labels, labels=list(range(len(labels))))
    print('\nClassification Report:')
    print(class_report)

    conf_matrix = confusion_matrix(y_true=y_true_mapped, y_pred=y_pred_mapped, labels=list(range(len(labels))))
    print('\nConfusion Matrix:')
    print(conf_matrix)

#Results of model before finetuning
evaluate(y_true[0:100], y_pred)



import bitsandbytes as bnb

def find_all_linear_names(model):
    cls = bnb.nn.Linear4bit
    lora_module_names = set()
    for name, module in model.named_modules():
        if isinstance(module, cls):
            names = name.split('.')
            lora_module_names.add(names[0] if len(names) == 1 else names[-1])
    if 'lm_head' in lora_module_names:
        lora_module_names.remove('lm_head')
    return list(lora_module_names)

modules = find_all_linear_names(model)
modules

from trl import SFTTrainer, SFTConfig

import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

peft_config = LoraConfig(
    lora_alpha=128,
    lora_dropout=0,
    r=32,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=modules,
)

training_arguments = SFTConfig(
    output_dir= '/outputs/',
    num_train_epochs=1,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=16,
    gradient_checkpointing=True,
    optim="paged_adamw_32bit",
    logging_steps=1,
    learning_rate=2e-4,
    weight_decay=0.001,
    fp16=True,
    bf16=False,
    max_grad_norm=0.3,
    max_steps=-1,
    warmup_ratio=0.03,
    group_by_length=False,
    lr_scheduler_type="cosine",
    report_to="wandb",
    eval_strategy="steps",
    eval_steps = 0.2
)

trainer = SFTTrainer(
    model=model,
    args=training_arguments,
    train_dataset=train_data,
    eval_dataset=eval_data,
    peft_config=peft_config,
    dataset_text_field="text",
    processing_class=tokenizer,
    max_seq_length=256,
    packing=False,
    dataset_kwargs={
    "add_special_tokens": False,
    "append_concat_token": False,
    }
)

trainer.train()



output_dir= '/outputs/'
trainer.save_model(output_dir)
tokenizer.save_pretrained(output_dir)

y_pred = predict(X_test[midpoint:], model, tokenizer)
evaluate(y_true[midpoint:], y_pred)

evaluate(y_true[midpoint:], y_pred)

