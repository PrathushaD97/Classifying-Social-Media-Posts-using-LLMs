# -*- coding: utf-8 -*-
"""HateXplain_BERT_Hate_Speech_Detection_WithTopic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v0xYWSi7Esnss64S4sH1M1Q8NZHIrjYY

## Task Description

> Hate Speech Detection is the automated task of detecting if a piece of text contains hate speech.

# Setting up the GPU Environment

#### Install Dependencies and Restart Runtime
"""

!pip install -q transformers
!pip install -q simpletransformers`
!pip install -U imbalanced-learn`

"""# Getting Data (Dataset: HateXplain"""

## Importing packages
import pandas as pd
import numpy as np
import requests
import json
import re

#Feching and storing the data
url = 'https://raw.githubusercontent.com/hate-alert/HateXplain/master/Data/dataset.json'
resp = requests.get(url)
data = json.loads(resp.text)
df = pd.DataFrame.from_dict(data, orient ='index')
df = df.reset_index(drop=True)

# Report the number of sentences.
print('Number of training sentences: {:,}\n'.format(df.shape[0]))

# Display 10 random rows from the data.
df.sample(10)

"""Once done we can take a look at the `head()` of the training set to check if our data has been retrieved properly."""

df['annotator_1_label'] = df['annotators'].apply(lambda num : num[0]['label'])
df['annotator_2_label'] = df['annotators'].apply(lambda num : num[1]['label'])
df['annotator_3_label'] = df['annotators'].apply(lambda num : num[2]['label'])

df['annotator_1_target'] = df['annotators'].apply(lambda num : num[0]['target'])
df['annotator_2_target'] = df['annotators'].apply(lambda num : num[1]['target'])
df['annotator_3_target'] = df['annotators'].apply(lambda num : num[2]['target'])

df['labels'] = df['annotator_1_label']+df['annotator_2_label']+df['annotator_3_label']
df['targets'] = df['annotator_1_target']+df['annotator_2_target']+df['annotator_3_target']

df['labels'] = df['labels'].apply(lambda num : str(num).lower().strip())
#df['targets'] = df['targets'].apply(lambda num : str(num).lower().strip())

df['text'] = df['post_tokens'].apply(lambda num : ' '.join(map(str,num)))

df = df.drop(columns=['annotators', 'rationales', 'post_tokens',
                      'annotator_1_label','annotator_2_label','annotator_3_label',
                      'annotator_1_target','annotator_2_target','annotator_3_target'])

#Function to take a list and return dictionary with count of items
def list_count(input_list):
    output_dict = {}
    for i in range(len(input_list)):
        output_dict[input_list[i]] = input_list.count(input_list[i])
    return(output_dict)

#Function to check if text is hate speech or offensive or neither
def speech_type(input_str):
    if input_str in ['hatespeechhatespeechnormal','hatespeechnormalhatespeech','normalhatespeechhatespeech',
                     'hatespeechoffensivehatespeech','offensivehatespeechhatespeech','hatespeechhatespeechoffensive',
                     'hatespeechhatespeechhatespeech']:
        val = 1
    elif input_str in ['offensivenormalhatespeech','hatespeechnormaloffensive','normalhatespeechoffensive',
                       'normaloffensivehatespeech','offensivehatespeechnormal','hatespeechoffensivenormal',
                       'offensivehatespeechoffensive','hatespeechoffensiveoffensive','offensiveoffensivehatespeech']:
        val = 0
    else:
        val = 0
    return val

#Creating Flags
df['is_hate'] = df['labels'].apply(speech_type)
df['target'] = df['targets'].apply(list_count)
df['target_count'] = df['target'].apply(lambda num: num.get('Asexual', 0)+num.get('Bisexual', 0)+num.get('Homosexual', 0))
df['is_lgbt_hate'] = df['target_count'].apply(lambda num: True if num>=2 else False)

# Removing LGBT hate labels for offensive and non offensive speech and drop unnecessary columns
for i in range(0,len(df)):
    if df['is_hate'][i]!=1:
        df['is_lgbt_hate'][i] = False

df = df.drop(columns=['labels','targets','target','target_count'])

#Create a function to clean text in the datafrmae
def clean_text(text):

  # Remove emojis
  text = re.sub(r'[:;][\w\d-]*[:;]', '', text)

  # Remove emoticons
  text = re.sub(r'\([^\)]*\)', '', text)

  # Remove hyperlinks
  text = re.sub(r'https?://[^\s]+', '', text)

  # Remove non-English characters
  text = re.sub(r'[^\w\s]', '', text)

  # Convert all text into lowercase
  text = text.lower()

  return text

df['text'] = df['text'].apply(clean_text)
df.sample(10)

df_hate = df.loc[df['is_hate']==1]

import sklearn
from sklearn.model_selection import train_test_split
train_df, test_df = train_test_split(df, test_size=0.15, random_state=123)

train_df[0:10]

"""We transform the dataframe column `label` so that the labels `hate` and `nothate` are now integers `1` and `0` respectively. This input format of labels is required for our training step with the `transformers` library."""

train_df = train_df.replace({'is_lgbt_hate': {True: 1, False: 0}}) # relabel the `label` column, hate is 1 and nothate is 0
test_df = test_df.replace({'is_lgbt_hate': {True: 1, False: 0}}) # relabel the `label` column, hate is 1 and nothate is 0
train_df.head()

"""We also rename the `label` column to `labels` as this also conforms to the input format required for the `simpletransformers` library."""

df['is_lgbt_hate'].value_counts()

train_df = train_df.rename(columns={'is_hate': 'labels'})
test_df = test_df.rename(columns={'is_hate': 'labels'})

train_df.head()

pip install simpletransformers

"""We can now take a look at the train and test set sizes. We see that this dataset is quite special as the `hate` and `nothate` class sizes are actually quite close in proportion."""

data = [[train_df.labels.value_counts()[0], test_df.labels.value_counts()[0]],
        [train_df.labels.value_counts()[1], test_df.labels.value_counts()[1]]]
# Prints out the dataset sizes of train test and validate as per the table.
pd.DataFrame(data, columns=['Train', 'Test'])

"""# Training and Testing the Model

# Detecting Hate Speech with BERT
"""

train_args = {
    'reprocess_input_data': True,
    'overwrite_output_dir': True,
    'sliding_window': True,
    'max_seq_length': 128,
    'num_train_epochs': 4,
    'train_batch_size': 128,
    'fp16': True,
    'output_dir': '/outputs/',
}

from simpletransformers.classification import ClassificationModel
import pandas as pd
import logging
import sklearn

logging.basicConfig(level=logging.DEBUG)
transformers_logger = logging.getLogger('transformers')
transformers_logger.setLevel(logging.WARNING)

# We use the XLNet base cased pre-trained model.
model = ClassificationModel('bert', 'bert-base-uncased', num_labels=2, args=train_args)

# Train the model, there is no development or validation set for this dataset
# https://simpletransformers.ai/docs/tips-and-tricks/#using-early-stopping
model.train_model(train_df)

# Evaluate the model in terms of accuracy score
result, model_outputs, wrong_predictions = model.eval_model(test_df, acc=sklearn.metrics.accuracy_score)
#accuracy
print(result)



#f1
result, model_outputs, wrong_predictions = model.eval_model(test_df, acc=sklearn.metrics.f1_score)
print(result)

#ROC AUC
result, model_outputs, wrong_predictions = model.eval_model(test_df, acc=sklearn.metrics.roc_auc_score)
print(result)

#Precision and Recall
result, model_outputs, wrong_predictions = model.eval_model(test_df, acc=sklearn.metrics.precision_recall_fscore_support)
print(result)

"""# Detect Target of Hate Speech with BERT"""

predictions, _ = model.predict(test_df['text'].tolist())
test_df['predictions'] = predictions
test_df.to_csv('results.txt', index=False, header=False) # saves the prediction results to a file in the colab environment
test_df[0:10]

test_df_hate = test_df.loc[test_df['predictions']==1]

test_df_hate = test_df_hate.drop(['labels','predictions'], axis =1)
test_df_hate = test_df_hate.rename(columns={'is_lgbt_hate': 'labels'})

test_df_hate.head()

train_df_hate = train_df.loc[train_df['labels']==1]

train_df_hate = train_df_hate.drop(['labels'], axis =1)
train_df_hate = train_df_hate.rename(columns={'is_lgbt_hate': 'labels'})
train_df_hate.head()

train_args = {
    'reprocess_input_data': True,
    'overwrite_output_dir': True,
    'sliding_window': True,
    'max_seq_length': 128,
    'num_train_epochs': 4,
    'train_batch_size': 128,
    'fp16': True,
    'output_dir': '/outputs/',
}

logging.basicConfig(level=logging.DEBUG)
transformers_logger = logging.getLogger('transformers')
transformers_logger.setLevel(logging.WARNING)

# We use the XLNet base cased pre-trained model.
model_lgbt = ClassificationModel('bert', 'bert-base-uncased', num_labels=2, args=train_args)

# Train the model, there is no development or validation set for this dataset
# https://simpletransformers.ai/docs/tips-and-tricks/#using-early-stopping
model_lgbt.train_model(train_df_hate)

# Evaluate the model in terms of accuracy score
result, model_outputs, wrong_predictions = model_lgbt.eval_model(test_df_hate, acc=sklearn.metrics.accuracy_score)
print(result)

#f1
result, model_outputs, wrong_predictions = model_lgbt.eval_model(test_df_hate, acc=sklearn.metrics.f1_score)
print(result)

#ROC_AUC
result, model_outputs, wrong_predictions = model_lgbt.eval_model(test_df_hate, acc=sklearn.metrics.roc_auc_score)
print(result)

#Precision and Recall
result, model_outputs, wrong_predictions = model_lgbt.eval_model(test_df_hate, acc=sklearn.metrics.precision_recall_fscore_support)
print(result)



"""# Topic Modelling with BERT"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install bertopic

from umap import UMAP

umap_model = UMAP(random_state=321)

from bertopic import BERTopic

topic_model = BERTopic(language="english", calculate_probabilities=True, verbose=True, umap_model=umap_model)



topic_df = train_df.loc[train_df['labels']==1]
topic_df.head()

topics, probs = topic_model.fit_transform(topic_df['text'])
freq = topic_model.get_topic_info(); freq.head(5)



def topic_label(row):
  val = '[Other]'
  qwords = ['fag', 'faggot', 'faggots', 'dyke', 'gay', 'lesbian', 'trans', 'dykes', 'lesbians', 'faggit','queer', 'queers', 'bisexual', 'bisexuals', 'transgender', 'trannies', 'gays']
  rwords = ['nigger', 'niggers', 'niggas', 'arabs', 'coons', 'coon', 'towelhead', 'niglet', 'niglets', 'raghead', 'spic', 'sand','rag', 'towel', 'chong', 'ching', 'chinese', 'sheeboon', 'beaner', 'beaners', 'mudsharks', 'mudshark', 'mudslimes', 'irish', 'ireland', 'zhids', 'libtard']
  lwords = ['muslim', 'muslims', 'moslem', 'muzzie', 'muzzies', 'muzrat', 'islam', 'kike', 'jew', 'jews']
  if any([item in row['Representation'] for item in qwords]):
    val = '[LGBTQ+]'
  elif any([item in row['Representation'] for item in rwords]):
    val = '[Race]'
  elif any([item in row['Representation'] for item in lwords]):
    val = '[Religion]'
  else:
    val = '[Other]'

  return val

freq['label_names'] = freq.apply(topic_label, axis =1)
freq.head()

dict_topic = pd.Series(freq.label_names.values,index=freq.Topic).to_dict()
topic_df['topics'] = topics
topic_df[0:10]

"""dict_topic = pd.Series(freq.label_names.values,index=freq.Topic).to_dict()
dict_topic
"""

topic_df['topic_label'] = topic_df['topics'].replace(dict_topic)

def add_words(row):
  words =['fag', 'faggot', 'faggots', 'dyke', 'dykes', 'lesbian', 'gay', 'trannie', 'trans', 'queer']
  value = row['topic_label']
  if any(x in row['text'] for x in words) and (row['topic_label'] =='[None]' or row['topic_label'] =='[Race]'):
    value = '[LGBTQ+]'
  return value

topic_df['topic_label'] =topic_df.apply(add_words, axis=1)
topic_df[0:10]

topic_df_train = topic_df.drop(['labels'], axis =1)
topic_df_train['text'] = topic_df_train['text'].map(str) + ' ' + topic_df_train['topic_label'].map(str)
topic_df_train = topic_df_train.rename(columns={'is_lgbt_hate': 'labels'})
topic_df_train.head()

topic_df_test = test_df_hate.copy()

topics2, probs2 = topic_model.fit_transform(topic_df_test['text'])

freq2 = topic_model.get_topic_info(); freq2.head()

freq2['label_names'] = freq2.apply(topic_label, axis =1)
freq2.head()

dict_topic2 = pd.Series(freq2.label_names.values,index=freq2.Topic).to_dict()
dict_topic2

topic_df_test['topics'] = topics2
topic_df_test['topic_label'] = topic_df_test['topics'].replace(dict_topic2)
topic_df_test['topic_label'] =topic_df_test.apply(add_words, axis=1)

topic_df_test['text'] = topic_df_test['text'].map(str) + ' ' + topic_df_test['topic_label'].map(str)

topic_df_test = topic_df_test.rename(columns={'is_lgbt_hate': 'labels'})

topic_df_test.head()



logging.basicConfig(level=logging.DEBUG)
transformers_logger = logging.getLogger('transformers')
transformers_logger.setLevel(logging.WARNING)

# We use the XLNet base cased pre-trained model.
model_target = ClassificationModel('bert', 'bert-base-uncased', num_labels=2, args=train_args)

# Train the model, there is no development or validation set for this dataset
# https://simpletransformers.ai/docs/tips-and-tricks/#using-early-stopping
model_target.train_model(topic_df_train)

# Evaluate the model in terms of accuracy score
result, model_outputs, wrong_predictions = model_target.eval_model(topic_df_test, acc=sklearn.metrics.accuracy_score)
print(result)

#f1
result, model_outputs, wrong_predictions = model_target.eval_model(topic_df_test, acc=sklearn.metrics.f1_score)
print(result)

#ROC_AUC
result, model_outputs, wrong_predictions = model_target.eval_model(topic_df_test, acc=sklearn.metrics.roc_auc_score)
print(result)

#Precision and Recall
result, model_outputs, wrong_predictions = model_target.eval_model(topic_df_test, acc=sklearn.metrics.precision_recall_fscore_support)
print(result)

predictions2, _ = model_target.predict(topic_df_test['text'].tolist())
topic_df_test['predictions'] = predictions2
topic_df_test.to_csv('topic_results.txt', index=False, header=False) # saves the prediction results to a file in the colab environment
topic_df_test[0:10]

wrong_predictions = topic_df_test.loc[topic_df_test['labels']!=topic_df_test['predictions']]

wrong_predictions.shape

wrong_predictions[0:10]

object_df = train_df.loc[train_df['labels']==1]
object_df.head()

#object of tweet
import spacy
nlp = spacy.load('en_core_web_sm')
def get_object_phrase(doc):
    for token in doc:
        if ("dobj" in token.dep_):
            subtree = list(token.subtree)
            start = subtree[0].i
            end = subtree[-1].i + 1
            return doc[start:end]

objects = []
for sentence in object_df['text']:
    doc = nlp(sentence)
    object_phrase = get_object_phrase(doc)
    objects.append(object_phrase)
object_df['object'] = objects
object_df.head()

object_df['text'] = object_df['text'].map(str) + ' [Object: ' + object_df['object'].map(str) + ' ]'
object_df = object_df.drop(['labels'], axis=1)
object_df = object_df.rename(columns={'is_lgbt_hate': 'labels'})

object_df.head()

object_df_test = test_df_hate.copy()
object_df_test.head()

test_objects = []
for sentence in object_df_test['text']:
    doc = nlp(sentence)
    object_phrase = get_object_phrase(doc)
    test_objects.append(object_phrase)
object_df_test['object'] = test_objects
object_df_test.head()

object_df_test['text'] = object_df_test['text'].map(str) + ' [Object: ' + object_df_test['object'].map(str) + ' ]'

logging.basicConfig(level=logging.DEBUG)
transformers_logger = logging.getLogger('transformers')
transformers_logger.setLevel(logging.WARNING)

# We use the XLNet base cased pre-trained model.
model_object = ClassificationModel('bert', 'bert-base-uncased', num_labels=2, args=train_args)

# Train the model, there is no development or validation set for this dataset
# https://simpletransformers.ai/docs/tips-and-tricks/#using-early-stopping
model_object.train_model(object_df)

# Evaluate the model in terms of accuracy score
result, model_outputs, wrong_predictions = model_object.eval_model(object_df_test, acc=sklearn.metrics.accuracy_score)
print(result)



#f1
result, model_outputs, wrong_predictions = model_object.eval_model(object_df_test, acc=sklearn.metrics.f1_score)
print(result)

#ROC_AUC
result, model_outputs, wrong_predictions = model_object.eval_model(object_df_test, acc=sklearn.metrics.roc_auc_score)
print(result)

#Precision and Recall
result, model_outputs, wrong_predictions = model_object.eval_model(object_df_test, acc=sklearn.metrics.precision_recall_fscore_support)
print(result)

object_topic_df = object_df.copy()

object_topic_df['topics'] = topics
object_topic_df['topic_label'] = object_topic_df['topics'].replace(dict_topic)
object_topic_df.head()

object_topic_df['text'] = object_topic_df['text'].map(str) + ' ' + object_topic_df['topic_label'].map(str)

object_topic_df_test = object_df_test.copy()

object_topic_df_test['topics'] = topics2
object_topic_df_test['topic_label'] = object_topic_df_test['topics'].replace(dict_topic)
object_topic_df_test['text'] = object_topic_df_test['text'].map(str) + ' ' + object_topic_df_test['topic_label'].map(str)
object_topic_df_test.head()

logging.basicConfig(level=logging.DEBUG)
transformers_logger = logging.getLogger('transformers')
transformers_logger.setLevel(logging.WARNING)

# We use the XLNet base cased pre-trained model.
model_object_topic = ClassificationModel('bert', 'bert-base-uncased', num_labels=2, args=train_args)

# Train the model, there is no development or validation set for this dataset
# https://simpletransformers.ai/docs/tips-and-tricks/#using-early-stopping
model_object_topic.train_model(object_topic_df)

# Evaluate the model in terms of accuracy score
result, model_outputs, wrong_predictions = model_object_topic.eval_model(object_topic_df_test, acc=sklearn.metrics.accuracy_score)
print(result)



#f1
result, model_outputs, wrong_predictions = model_object_topic.eval_model(object_topic_df_test, acc=sklearn.metrics.f1_score)
print(result)

#ROC_AUC
result, model_outputs, wrong_predictions = model_object_topic.eval_model(object_topic_df_test, acc=sklearn.metrics.roc_auc_score)
print(result)

#Precision and Recall
result, model_outputs, wrong_predictions = model_object_topic.eval_model(object_topic_df_test, acc=sklearn.metrics.precision_recall_fscore_support)
print(result)

"""# Using the Model (Running Inference)

Running the model to do some predictions/inference is as simple as calling `model.predict(input_list)`.
"""

samples = [test_df[test_df['labels'] == 0].sample(1).iloc[0]['text']] # get a random sample from the test set which is nothate
predictions, _ = model.predict(samples)
label_dict = {0: 'nothate', 1: 'hate'}
for idx, sample in enumerate(samples):
  print('{} - {}: {}'.format(idx, label_dict[predictions[idx]], sample))

"""We can also generate a `results.txt` file from the test set. The file is stored in our Colab environment. Hit the `folder` icon at the side and you can download the `results.txt` file from the file browser. You can submit this `.txt` file to [Dynabench](https://dynabench.org/tasks/5#overall) for evaluation if you wish to."""

predictions, _ = model.predict(test_df['text'].tolist())
test_df['predictions'] = predictions
test_df.to_csv('results.txt', index=False, header=False) # saves the prediction results to a file in the colab environment
test_df[0:30]

"""We can connect to Google Drive with the following code to save any files you want to persist. You can also click the `Files` icon on the left panel and click `Mount Drive` to mount your Google Drive.

The root of your Google Drive will be mounted to `/content/drive/My Drive/`. If you have problems mounting the drive, you can check out this [tutorial](https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166).

More Notebooks @ [eugenesiow/practical-ml](https://github.com/eugenesiow/practical-ml) and do drop us some feedback on how to improve the notebooks on the [Github repo](https://github.com/eugenesiow/practical-ml/).
"""